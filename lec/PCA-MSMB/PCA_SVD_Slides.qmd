---
title: "Multivariate Analysis"
subtitle: "BDS^3 Uzhhorod, Ukraine"
author: "Wolfgang Huber, Susan Holmes. Updates by Helena Crowell"
date: "July 2-14, 2023"
execute: 
  echo: true
  cache: false
  message: false
  warning: false 
format:
  revealjs:
    dpi: 160
    fontsize: 18pt
    df-print: kable
    fig-width: 5
    fig-height: 5
    fig-align: center
    scrollable: true
    highlight-style: pygments
---

```{r initial, include=FALSE}
options(digits=3, width=80, prompt=" ", continue=" ")
```

# Matrices and their Motivation

```{r echo=FALSE}
library(BiocFileCache)
bfc <- BiocFileCache(tempfile(), ask=FALSE)
url <- "https://www.huber.embl.de/msmb/data.tar.gz"
untar(bfcrpath(bfc, url))
```

It is current practice to measure many variables on the same patients,  
we may have all the biometrical characteristics, height, weight, BMI,  
age as well as clinical variables such as blood pressure, blood sugar,  
heart rate for 100 patients, these variables will not be independent.

![](resources/TheMatrix.jpg)

# What are the data?

- To start off, a useful toy example we’ll use is from the sports world;  
  performances of decathlon athletes.

  These are measurements of athletes’ performances in the decathlon:  
  the variables `m100`, `m400`, `m1500` are performance times in seconds for the  
  100 metres, 400 metres and 1500 meters, respectively, `m110` is the time taken  
  to finish the 110 meters hurdles, `pole` is the pole-jump height, and `weight`  
  is the length in metres the athletes threw the weight.
    
```{r loadath, echo=FALSE}
library(ade4)
data(olympic)
athletes <- olympic$tab
#athletes <- scale(athletes)
#athletes <- data.frame(athletes)
names(athletes) <- c("m100", "long", "weight", "highj",
    "m400", "m110", "disc", "pole", "javel", "m1500")
format(head(athletes), digits=3, nsmall=2)
```

## Diabetes

- Clinical measurements (`diabetes` data). This data measures glucose  
  levels in the blood after fasting (`glufast`), after a test condition  
  (`glutest`) as well as steady state plasma glucose (`steady`) and steady  
  state (`insulin`) for diabetes, the sixth variable is not continuous  
  and is considered a *supplementary* variable as we will see.

```{r}
diabetes <- read.table("data/diabetes.txt", header=TRUE, row.names=1)
diabetes[1:4, ]
```

## Microbial Ecology

- Operational Taxon Unit read counts in a microbial ecology study;  
  the columns represent different ‘species’ of bacteria,  
  the rows are labeled for the samples.

                     469478 208196 378462 265971 570812
        EKCM1.489478      0      0      2      0      0
        EKCM7.489464      0      0      2      0      2
        EKBM2.489466      0      0     12      0      0
        PTCM3.489508      0      0     14      0      0
        EKCF2.489571      0      0      4      0      0
        
## RNA-Seq

- Here are some RNA-seq transcriptomic data showing numbers of mRNA  
  reads present for different patient samples, the rows are patients  
  and the columns are the genes.

                   FBgn0000017 FBgn0000018 FBgn0000022 FBgn0000024 FBgn0000028 FBgn0000032
        untreated1        4664         583           0          10           0        1446
        untreated2        8714         761           1          11           1        1713
        untreated4        3150         310           0           3           0         672
        treated1          6205         722           0          10           0        1698
        treated3          3334         308           0           5           1         757
        
## Mass Spectroscopy

- Mass spectroscopy data where we have samples containing  
  informative labels (knockout versus wildtype mice) and  
  protein $\times$ features designated by their m/z number.

        mz       129.9816   72.08144  151.6255  142.0349  169.0413    186.0355
        KOGCHUM1  60515      181495          0    196526    25500     51504.40
        WTGCHUM1 252579       54697        412    487800    48775    130491.15
        WTGCHUM2 187859       56318      46425    454226    45626    100845.01
        

## Expression Data (microarray)

- Here the rows are samples from different subjects and different  
  T cell types and the columns are a subset of gene expression  
  measurements on the 156 most differentially expressed genes.
  
```{r}
# Melanoma/Tcell Data: Peter Lee, Susan Holmes, PNAS.
load("data/Msig3transp.RData")
round(Msig3transp, 2)[1:5, 1:6]
```

## Biometrical Measurements

- Measurements: turtles

```{r}
turtles <- read.table("data/PaintedTurtles.txt", header=TRUE)
turtles[1:4, ]
```

- Some biological traits of lizards are available in the `ade4` package

```{r}
require(ade4)
data(lizards)
lizards$traits[1:4, c(1,5,6,7,8)]
```

# Data visualization and preparation

It is always beneficial to start a multidimensional analysis  
by checking the simple one dimensional and two dimensional  
summary statistics, we can visualize these using a graphics  
package that builds on `ggplot2` called `GGally`.

## Low dimensional data summaries and preparation

What do we mean by low dimensional?

![](resources/flatland.png){width="80.00000%"}

If we are studying only one variable, just one column of our matrix,  
we might call it ${\mathbf x}$ or ${\mathbf x}_{\bullet j}$;
we call it one dimensional.

A *one dimensional summary* a histogram that shows that variable’s distribution,  
or we could compute its mean $\bar{x}$ or median, these are zero-th dimensional  
summaries of one dimension data.

In lecture 3 we studied two dimensional scatterplots. 

When considering two variables ($x$ and $y$) measured together on a set of observations,  
the **correlation coefficient** measures how the variables co-vary.

This is a single number summarizes two dimensional data,
its formula involves $\bar{x}$ and $\bar{y}$:
$$\hat{\rho}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}
\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}
\label{eq:corrcoeff}$$ 

```{r corrturt}
cor(turtles[, -1])
```

```{r turtlespairs,fig.height=3,fig.width=3}
library(GGally)
ggpairs(turtles[, -1], axisLabels="none")
```

```{r PairsAthletes,fig.width=4.5,fig.height=4.5}
ggpairs(athletes)
```

```{r heatmapathletes, fig.width=4.2,fig.height=3.5}
library(pheatmap)
pheatmap(cor(athletes), cell.width=10, cell.height=10)
```

# A Little History

Invented in 1901 by Karl Pearson as a way to reduce  
a two variable scatterplot to a single coordinate.

Used by statisticians in the 1930s to summarize a battery of psychological    
tests run on the same subjects Hotelling:1933, extracting overall scores  
that could summarize many variables at once.

It is called Principal Component Analysis (abbreviated PCA). 

**Not principled.**

# Dimension reduction

:::: {layout-ncol=2}

::: {.column}

PCA is an ‘unsupervised learning technique’  
because it treats all variables as having  
the same status.

PCA is visualization technique which produces  
maps of both variables and observations.

We are going to give you a flavor of  
what is called multivariate analyses.  

As a useful first approximation, we  
formulate many of the methods through  
manipulations called linear algebra.

The *raison d’être* for multivariate analyses  
is connections or associations between  
the different variables. 

:::

::: {.column}

If the columns of the matrix are unrelated,  
we should just study each column separately  
and do standard univariate statistics on them  
one by one.

Use geometry:

![](resources/projectionvector.jpg)

:::

::::

## Low Dimensional Projections

Here we show one way of projecting two dimensional data onto a line. 

The olympic data come from the `ade4` package, they are the  
performances of decathlon athletes in an olympic competition.

```{r SS, echo=FALSE}
require(ade4)
require(ggplot2)
require(grid)
data(olympic)
athletes <- olympic$tab
athletes <- scale(athletes)
athletes <- data.frame(athletes)
names(athletes) <- c("m100", "long", "weight", "highj", 
  "m400", "m110", "disc", "pole", "javel", "m1500")
p <- ggplot(athletes, aes(x=weight, y=disc))
p <- p + geom_point(size=3, shape=21)
pr <- p + geom_rug(sides="b", colour="red")
print(pr)
```

Scatterplot of two variables showing projection on the x coordinate in red.

# How do we summarize two <br> dimensional data by a line?

In general, we lose information about the points when we   
project down from two dimensions (a plane) to one (a line). 

If we do it just by using the original coordinates, e.g., the x coordinate  
as we did above, we lose all the information about the second one.

There are actually many ways of projecting the point cloud  
onto a line. One is to use what are known as regression lines.  
Let’s look at these lines and how there are constructed in R:

## Regressing one variable on the other

### The disc variable on the weight

:::: {layout-ncol=2}

::: {.column}

```{r Reg1}
#| fig-show: hide
#| dependson: SS
attach(athletes)
require(ggplot2)
reg1 <- lm(disc~weight, data=athletes)
a <- reg1$coefficients[1] # intercept
b <- reg1$coefficients[2] # slope
pline <- p + geom_abline(intercept=a, slope=b, col="blue")
proj <- pline + geom_segment(aes(
  x=weight, xend=weight,
  y=disc, yend=reg1$fitted),
  linetype=1, colour="red", 
  arrow=arrow(length=unit(0.15, "cm")))
print(proj)
```

The blue line minimizes the sum of squares  
of the vertical residuals (in red). 

<br>

What is the variance of the points  
along the blue line?

```{r}
matproj <- cbind(weight, reg1$fitted)
sum(apply(matproj, 2, var))
```

:::

::: {.column}

```{r echo=FALSE}
proj
```

:::

::::

--- 

### Regression of weight on discus

```{r Reg2,echo=FALSE}
require(ggplot2)
reg2 <- lm(weight~disc, data=athletes)
#abline(reg1, col='red')
a2 <- reg2$coefficients[1] # Intercept
b2 <- reg2$coefficients[2] # slope
#abline(-a/b , 1/b, col="blue")
pline2 <- p + geom_abline(
  intercept=-a2/b2, slope=1/b2, 
  col="green", linetype=1, lwd=2)
proj2 <- pline2 + geom_segment(
  aes(x=weight, xend=reg2$fitted, y=disc, yend=disc),
  linetype=1,colour="orange", arrow=arrow(length=unit(0.15,"cm")))
print(proj2)
```

Variance of points:

```{r}
matproj2 <- cbind(weight, reg2$fitted)
sum(apply(matproj, 2, var))
```

The orange line minimizes the horizontal residuals for the weight variable in orange.

## The PCA line: it minimizes in both directions

```{r PCAmin-code, fig.show='hide'}
xy <- cbind(athletes$disc, athletes$weight)
svda <- svd(xy)
pc <- xy %*% svda$v[, 1] %*% t(svda$v[ ,1])
bp <- svda$v[2, 1] / svda$v[1, 1]
ap <- mean(pc[, 2]) - bp * mean(pc[, 1])
p <- p + 
  geom_segment(xend=pc[,1], yend=pc[, 2]) +
  geom_abline(intercept=ap, slope=bp, col="purple", lwd=1.5)
p
```

::: {layout-ncol=3}

```{r PCAmin-plot, fig.width=3, fig.height=3, echo=FALSE}
p
```

```{r PCAR1R2, fig.width=3, fig.height=3, echo=FALSE}
pline + coord_fixed() +
  geom_segment(aes(xend=weight, yend=reg1$fitted), colour="blue", alpha=0.35)+
  geom_abline(intercept=-a2/b2, slope=1/b2, col="green", lwd=1.5, alpha=0.8) +
  geom_segment(aes(xend=reg2$fitted, yend=disc), colour="orange", alpha=0.35) +
  geom_abline(intercept=ap, slope=bp, col="purple", lwd=1.5, alpha=0.8) +
  geom_segment(xend=pc[, 1], yend=pc[, 2], colour="purple", alpha=0.35) 
```

<br> 
The purple line minimizes  
both residuals and thus  
(through Pythagoras) it  
minimizes the sum of  
squared distances from  
the points to the line.

:::

*Minimizing the distance to the line in both directions,  
the purple line is the principal component line,  
the green and blue lines are the regression lines.*

# Variance along the line

The lines created here are sensitive to the choice of units;  
because we have made the standard deviations equal to one for both variables,  
the PCA line is the diagonal that cuts exactly in the middle of both regression lines. 

The data were centered by subtracting their means, thus   
ensuring that the line passes through the origin $(0,0)$.

Compute the variance of the points on the purple line.

The coordinates of the points when we made the plot, 
these are in the `pc` vector:

```{r PCAvari}
apply(pc, 2, var)
sum(apply(pc, 2, var))
```

## PCA for 2 dimensional data

```{r CompareSDs-code, fig.show='hide'}
ppdf <- data.frame(
  PC1n=-svda$u[, 1]*svda$d[1], 
  PC2n= svda$u[, 2]*svda$d[2])
p <- ggplot(ppdf, aes(x=PC1n,y=PC2n)) + 
  geom_point() + xlab("PC1") + ylab("PC2") +
  xlim(-3.5, 2.7) + ylim(-2,2) + coord_fixed() +
  geom_hline(yintercept=0, color="purple", lwd=1.5, alpha=0.5) +
  geom_point(aes(x=PC1n, y=0), color="red") + 
  geom_segment(aes(xend=PC1n, yend=0), color="red") 
p
```

::: {layout-ncol=2}

```{r CompareSDs-plot, echo=FALSE}
p
```

```{r CompareSDblue, echo=FALSE}
segs1 <- apply(cbind(rep(0, 33), ppdf[, 1]), 1, min)
segs2 <- apply(cbind(rep(0, 33), ppdf[, 1]), 1, max)
segm <- data.frame(xmin=segs1, xmax=segs2, 
  yp=seq(-1, -2, length=33), yo=ppdf$PC2n)
ggplot(ppdf, aes(x=PC1n, y=PC2n)) + 
  geom_point() + ylab("PC2") + xlab("PC1") +
  geom_hline(yintercept=0, color="purple", lwd=1.5, alpha=0.5) +
  geom_point(aes(x=PC1n,y=0),color="red") +
  xlim(-3.5, 2.7) + ylim(-2,2) + coord_fixed() +
  geom_segment(aes(xend=PC1n,yend=0), color="red")+
  geom_segment(data=segm, aes(x=xmin, xend=xmax, y=yo, yend=yo), color="blue", alpha=0.5)
```

:::

# Notes about Lines

The line created here is sensitive to the choice of units, and to the center of the cloud.

Note that Pythagoras’ theorem tells us two interesting things here:     
if we are minimizing in both horizontal and vertical directions, we are  
in fact nminimizing the diagonal projections onto the line from each point.

## Principal Components are Linear Combinations <br> of the ‘old’ variables

To understand what that a linear combination really is, we can take  
an analogy, when making a healthy juice mix, you can follow a recipe.

::: {layout-ncol=2}

![](resources/Vegetable-Juice.jpg)

![](resources/RecipeVeggie.jpg)

:::

$$V = 2\times \mbox{ Beets } 
+ 1 \times \mbox{ Carrots }
+ \frac{1}{2} \mbox{ Gala } 
+ \frac{1}{2} \mbox{ GrannySmith }
+ 0.02 \times \mbox{ Ginger } 
+ 0.25 \mbox{ Lemon }$$ 

This recipe is a linear combination of individual juice types. In our analogy,  
these are replaced by the original variables. The result is a new variable,  
the coefficients $(2,1,\frac{1}{2},\frac{1}{2},0.02,0.25)$ are called the loadings.

# Optimal lines

A linear combination of variables defines a line in our space in the  
same way we say lines in the scatterplot plane for two dimensions.   
As we saw in that case, there are many ways to choose lines onto which  
we project the data, there is however a ‘best’ line for our purposes.

**Total variance can be decomposed:**  
The total sums of squares of the distances between the points and any line   
can be decomposed into the distance to the line and the variance along the line.

We saw that the principal component minimizes the distance to the line,  
and it also maximizes the variance of the projections along the line.

# Good Projections

What is this?

![](resources/CAM3.png)

# Good Projections

Which projection do you think is better?

![](resources/CAM4.png)

It’s the projection that maximizes the area of the shadow.  
An equivalent measurement is the sums of squares of the distances between points  
in the projection: we want to see as much of the variation as possible -- that’s what PCA does.

# The PCA workflow

::: {layout-ncol=2}

PCA is based on the principle of finding  
the largest axis of inertia/variability,  
and then iterating to find the next best axis  
that is orthogonal to the previous one and so on. 

![Many Choices have to made during PCA processing.](resources/smallorgacp1.png)

:::

## The Inner Workings of PCA: <br> the Singular Value Decomposition

Eigenvalues of X'X or Singular values of X tell us the rank.

:::: {layout-ncol=2}

::: {.column width="40%"}

What does rank mean?

```
   X |  2  4  8  
  ---| --------
   1 | 
   2 |
   3 |
   4 |
```
```
  X  |  2  4  8  
  -- | ---------
  1  |  2
  2  |  4
  3  |  6
  4  |  8
```
```
  X  |  2  4  8  
  ---| --------
   1 |  2  4  8
   2 |  4  8 16
   3 |  6 12 24
   4 |  8 16 32 

```

:::

::: {.column width="60%"}

We say that the matrix 

$$
\begin{pmatrix}
2 &  4 &  8 \\ 
4 &  8 & 16 \\
6 & 12 & 24 \\
8 & 16 & 32 \\
\end{pmatrix}
$$

is of rank one.

::: 

::::

$$
\begin{pmatrix}
2 &  4 &  8 \\ 
4 &  8 & 16 \\
6 & 12 & 24 \\
8 & 16 & 32 \\
\end{pmatrix}
== u * t(v) = u * v', \qquad
u = \left(\begin{smallmatrix}
1 \\ 2\\ 3 \\ 4
\end{smallmatrix}\right) 
\mbox{ and } v'=t(v)=(2\;4\;8) .
$$

## Backwards from the matrix to decomposition

```{r}
X <- matrix(c(
  780,  75,  540, 936, 
   90, 648, 1300, 125,
  900, 728,   70, 504), nrow=3)
X
u1 <- c(0.8, 0.1, 0.6)
v1 <- c(0.4, 0.5, 0.7, 0.4)
sum(u1^2)
sum(v1^2)
s1 <- 2348.2
s1 * u1 %*% t(v1)
X - s1 * u1 %*% t(v1)
```

<!-- ???
# Graphical Decompositions

```{r}
# <img src="/Users/susan/Dropbox/images/testsmallmosaic.jpg" alt="Decompose1" style="width: 400px;"/>
# 
# Matrix $X$ we would like to decompose.
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot1.png" alt="Decompose1" style="width: 400px;"/>
# 
# Areas are proportional to the  entries
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot2.png" alt="Decompose2" style="width: 400px;"/>
# 
# Looking at different possible margins
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot3.png" alt="Decompose3" style="width: 400px;"/>
```

Forcing the margins to have norm $1$
-->

## Check with R

```{r}
## check X
u1 <- c(0.8196, 0.0788, 0.5674)
v1 <- c(0.4053, 0.4863, 0.6754, 0.3782)
s1 <- 2348.2
s1 * u1 %*% t(v1)
```

```{r}
Xsub <- matrix(c(12.5, 35, 25, 25, 9, 14, 26, 18, 16, 21, 49, 
  32, 18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol=4, byrow=TRUE)
Xsub
USV <- svd(Xsub)
USV
```

```{r}
## check USV
Xsub-(135*USV$u[,1]%*%t(USV$v[,1]))
Xsub-(135*USV$u[,1]%*%t(USV$v[,1]))-(28.1*USV$u[,2]%*%t(USV$v[,2]))
Xsub-USV$d[1]*USV$u[,1]%*%t(USV$v[,1])-USV$d[2]*USV$u[,2]%*%t(USV$v[,2])
```

## Another Example

```{r}
Xsub <- matrix(c(12.5, 35, 25, 25, 9, 14, 26, 18, 16, 21, 49,
  32, 18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol=4, byrow=TRUE)
Xsub
svd(Xsub)
```

```{r}
USV <- svd(Xsub)
XS1 <- Xsub - USV$d[1] * (USV$u[, 1] %*% t(USV$v[, 1]))
XS1
XS2 <- XS1 - USV$d[2] * (USV$u[, 2] %*% t(USV$v[, 2]))
XS2
```

## Special Example of Rank one matrix: independence

```{r}
require(ade4)
HairColor <- HairEyeColor[, , 2]
HairColor
chisq.test(HairColor)
prows <- sweep(HairColor, 1, apply(HairColor, 1, sum), "/")
pcols <- sweep(HairColor, 2, apply(HairColor, 2, sum), "/")
Indep <- 313 * as.matrix(prows) %*% t(as.matrix(pcols))
round(Indep)
sum((Indep-HairColor)^2 / Indep)
```

## SVD for real data

```{r}
diabetes.svd <- svd(scale(diabetes[, -5]))
names(diabetes.svd)
diabetes.svd$d
```

```{r}
turtles.svd <- svd(scale(turtles[, -1]))
turtles.svd$d
```

# SVD

```{r fig.width=10, fig.height=6, echo=FALSE}
library(png)
library(grid)
#img <- readPNG("/Users/susan/Books/CUBook/images/SumRankOneD.png")
 #grid.raster(img)
```

$$
{\Large X
= u_{\bullet 1} * s_1 * v_{\bullet 1}
+ u_{\bullet 2} * s_2 * v_{\bullet 2}
+ u_{\bullet 3} * s_3 * v_{\bullet 3}
+\dots}
$$

We write our horizontal/vertical decomposition of the matrix $X$ in short hand as:

$${\Large X = USV', V'V=I, U'U=I, S}$$

(*diagonal matrix of singular values, given by the `d` component in the R function)

<br>

The crossproduct of X with itself verifies

$${\Large X'X=VSU'USV'=VS^2V'=V\Lambda V'}$$

where $V$ is called the eigenvector matrix of the symmetric matrix $X'X$  
and $\Lambda$ is the diagonal matrix of eigenvalues of $X'X$.

# Why Eigenvectors are useful?

![Why would eigenvectors come into use in Cinderella?](resources/xkcdEigenVectors.png)

[Khan's Academy](https://www.khanacademy.org/math/linear-algebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-eigenvalues-and-eigenvectors)

# Principal Components

The singular vectors from the singular value decomposition,  
`svd` function above, tell us the coefficients to put in front  
of the old variables to make our new ones with better properties.  

We write this as:

$$PC_1=c_1 X_{\bullet 1} +c_2 X_{\bullet 2}+ c_3 X_{\bullet 3}+\cdots c_p X_{\bullet p}$$

Replace $X_{\bullet 1},X_{\bullet 2}, \ldots X_{\bullet p}$ by $$PC_1, PC_2, \ldots PC_k$$

## What is the largest k can be ?

Suppose we have 5 samples with 23,000 genes measured   
on them, what is the dimensionality of these data?

The number of principal components is less than   
or equal to the number of original variables. 

$$K\leq min(n,p)$$

The geometr(ies) of data: good trick look at size of vectors:

```{r fig.width=9, fig.height=4,echo=FALSE}
library(png)
library(grid)
#img <- readPNG("/Users/susan/Books/CUBook/images/DataCloudGeometry.png")
# grid.raster(img)
```

```{r fig.width=9, fig.height=4,echo=FALSE}
# library(png)
# library(grid)
# img <- readPNG("/Users/susan/Books/CUBook/images/DataCloudGeometry4.png")
#  grid.raster(img)
```

The Principal Component transformation is defined in such a way that 

- The first principal component has the largest possible variance  
  (that is, accounts for as much of the variability in the data as possible). 
- Each successive component in turn has the highest variance possible  
  under the constraint that it be orthogonal to the preceding components. 

$$\max_{aX} \mbox{var}(Proj_{aX} (X))$$

Suppose the matrix of data $X$ has been made  
to have column means 0 and standard deviations 1.

# Matrix Decomposition

We call the principal components the columns of the matrix, $C=US$.

The columns of U (the matrix given as USV\$u in the output from the `svd` function above)  
are rescaled to have norm $s^2$, the variance they are responsable for.

If the matrix $X$ comes from the study of $n$ different samples or specimens,  
then the principal components provides new coordinates for these $n$ points.  
these are sometimes also called the scores in some of the (many) PCA functions  
available in R (`princomp`,`prcomp`,`dudi.pca` in `ade4`).

# Transition Formulae

If we only want the first one then it is just $c_1=s_1 u_1$.

Variance explained by first principal component: $s_1^2$:

Notice that $||c_1||^2=s_1'u_1 u_1' s_1= s_1^2 u_1'u_1=s_1^2=\lambda_1$

$$X'C=VSU'US=VS^2$$

# Remarks:

1. Each principal component is chosen to maximize the variance it  
  explains; this variance is measured by the corresponding eigenvalue.      
2. The new variables are made to be orthogonal, if the data.  
  are multivariate normal the new variables will be independent.      
3. When the variables are rescaled or we choose the correlation matrix  
  as the one we want to study instead of the covariance matrix,  
  then the sum of the variances of all the variables is the  
  number of variables (= p), this is sometimes called the trace.       
4. The principal components are always ordered by `importance',  
  always look at what proportion of the variability you are interpreting  
  (and check the screeplot before deciding how many components).
  
# A few examples of using PCA

We start with the turtles data that has 3 continuous variables and  
a gender variable that we leave out for the original PCA analysis.

## Turtles Data

When computing the variance covariance matrix, many programs use $1/(n-1)$ as the denominator.     
Here, $n=48$, so the sum of the variances are off by a small fudge factor of $48/47$.

```{r PCAturtlesunscaled}
turtles3var <- turtles[, -1]
apply(turtles3var, 2, mean)
turtles.pca <- princomp(turtles3var)
print(turtles.pca)
(25.06^2 + 2.26^2 + 1.94^2) * (48/47)
apply(turtles3var, 2, var)
```

```{r PCAturtles}
apply(turtles[, -1], 2, sd)
turtlesc <- scale(turtles[, -1])
cor(turtlesc)
pca1 <- princomp(turtlesc)
pca1
```

# Step one: always the screeplot

The screeplot showing the eigenvalues for the standardized data:  
one very large component in this case and two very small ones,  
the data are (almost) one dimensional.

```{r turtlesbiplot}
pca.turtles <- dudi.pca(turtles[, -1], scannf=FALSE, nf=2)
scatter(pca.turtles)
```

# Why ?

Choose k carefully:

```{r screeploteq, echo=FALSE, results=FALSE}
library("factoextra")
load("data/screep7.RData")
pcaS7 <- dudi.pca(screep7, scannf=FALSE)
fviz_eig(pcaS7, geom="bar", width=0.3)
#problem with dudi and prcomp eigenvalues
#prcomp does not scale by default, dudi.pca does
#fviz_eig(pcaS7,geom="bar",width=0.3)
#p7=prcomp(screep7,scale= TRUE)
#p7$sdev^2
#plot(p7)
```

# Step Two: Variables

```{r turtlesCircle,echo=FALSE}
require(ggplot2)
circle <- function(center=c(0, 0), npoints=100) {
    r <- 1
    tt <- seq(0, 2 * pi, length=npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[1] + r * sin(tt)
    return(data.frame(x=xx, y=yy))
}
corcir <- circle(c(0, 0), npoints=100)
pca1 <- prcomp(turtlesc, scale.=TRUE)
# create data frame with correlations between variables and PCs
correlations <- as.data.frame(cor(turtlesc, pca.turtles$li))

# data frame with arrows coordinates
arrows <- data.frame(
  x1=c(0, 0, 0), x2=correlations$Axis1,
  y1=c(0, 0, 0), y2=correlations$Axis2)

# geom_path will do open circles
ggplot() + 
  geom_path(data=corcir, aes(x=x, y=y), colour="gray65") + 
  geom_segment(data=arrows, aes(x=x1, y=y1, xend=x2, yend=y2), colour="gray65") + 
  geom_text(data=correlations, aes(x=Axis1, y=Axis2, label=rownames(correlations))) + 
  geom_hline(yintercept=0, colour="gray65") + 
  geom_vline(xintercept=0, colour="gray65") + 
  xlim(-1.1, 1.1) + ylim(-1.1, 1.1) + 
  labs(x="PC1 Axis", y="PC2 axis") + 
  ggtitle("Circle of correlations")
```

# Biplot

```{r turtlesbiplotfunction,echo=FALSE}
PCbiplot <- function(outputfromdudi, x="Axis1", y="Axis2") {
  # outputfromdudi being a dudi object
  PC <- outputfromdudi
    
  data <- data.frame(obsnames=row.names(PC$li), PC$li)
  plot <- ggplot(data, aes_string(x=x, y=y)) + geom_text(alpha=.4, size=3, aes(label=obsnames))
  plot <- plot + geom_hline(aes(yintercept=0), size=.2) + geom_vline(aes(xintercept=0), size=.2)
  dimnames(PC$co)[[2]] <- c(x,y)    
  datapc <- data.frame(varnames=rownames(PC$co), PC$co)
    mult <- min(
        (max(data[,y]) - min(data[,y])/(max(datapc[,y])-min(datapc[,y]))),
        (max(data[,x]) - min(data[,x])/(max(datapc[,x])-min(datapc[,x]))))
    datapc <- transform(datapc,
            v1=.8 * mult * (get(x)),
            v2=.8 * mult * (get(y)))
    plot <- plot + geom_text(data=datapc, 
      aes(x=v1, y=v2, label=varnames), 
      size=5, vjust=1, color="red") +
      coord_equal()
    plot <- plot + geom_segment(
      data=datapc, aes(x=0, y=0, xend=v1, yend=v2), 
      arrow=arrow(length=unit(0.2,"cm")), alpha=0.75, color="red")
    plot
}
PCbiplot(pca.turtles)
```

# All together

```{r scatterturtles}
scatter(pca.turtles)
```

Exercise: How are the following  numbers related?

```{r}
svd(turtlesc)$d / pca1$sdev
sqrt(47)
nrow(turtlesc)
```

## Lizards Data Analyses

This data  set describes 18 lizards as reported by Bauwens and D\'iaz-Uriarte (1997).   
It also gives life-history traits corresponding to these 18 species.

- `mean.L` (mean length (mm)), `matur.L` (length at maturity (mm)),    
- `max.L` (maximum length (mm)), `hatch.L` (hatchling length (mm)),     
- `hatch.m` (hatchling mass (g)), `clutch.S` (Clutch size),      
- `age.mat` (age at maturity (number of months of activity)),    
- `clutch.F` (clutch frequency).

```{r, echo=TRUE}
library(ade4)
data(lizards)
names(lizards)
lizards$traits[1:4, ]
```

It is always a good idea to check the variables one at a time and 
two at a time to see what the basic statistics are for the data

```{r}
tabtraits <- lizards$traits
options(digits=2)
colMeans(tabtraits)
cor(tabtraits)
```

## Biplot

```{r lizardbiplot}
require(ade4)
res <- dudi.pca(tabtraits, scannf=FALSE, nf=2)
```

<br>

::: {layout-ncol=3}

```{r}
res
```

```{r}
barplot(res$eig)
```

```{r}
biplot(res)
```

:::

```{r}
res$eig / (sum(res$eig))
```

# The Decathlon Athletes

```{r ade4athletes}
round(cor(athletes),1)
pca.ath <- dudi.pca(athletes, scan=FALSE)
pca.ath$eig
barplot(pca.ath$eig)
```

The screeplot is the first thing to look at, it tells us  
that it is satisfactory to use a two dimensional plot.

## Correlation Circle

The correlation circle made by showing the projection of  
the old variables onto the two first new principal axes:

:::: {layout-ncol=2}

::: {.column width="50%"}

```{r athletecorr, fig.width=3.5, fig.height=3.5}
s.corcircle(pca.ath$co, clab=1, grid=FALSE, fullcircle=TRUE, box=FALSE)
```

:::

::: {.column width="50%"}

```{r atheleteneg}
athletes[, c(1,5,6,10)] <- -athletes[, c(1,5,6,10)]
round(cor(athletes), 1)
```

<br>

```{r}
pcan.ath <- dudi.pca(athletes, scannf=FALSE, nf=2)
pcan.ath$eig
```

:::

::::

Now all the negative correlations are quite small ones.  
Doing the screeplot over again will show no change in the eigenvalues,  
the only thing that changes is the sign of loadings for the m variables.

# New Data changing signs

```{r athletecorrn}
s.corcircle(pcan.ath$co, clab=1.2, box=FALSE)
```

*Correlation circle after changing the signs of the running variables.*

# Observations

```{r, athletepc,echo=FALSE,fig.width=4.2, fig.height =3.6}
ggplot(pcan.ath$l1, aes(x=RS1, y=RS2, 
  label=rownames(pcan.ath$l1))) + 
  geom_text() + coord_fixed() +
  xlim(-3,3) + ylim(-2.5,3) + 
  geom_hline(yintercept=0,linetype=2) + 
  geom_vline(xintercept=0,linetype=2) 
```

```{r}
data(olympic)
olympic$score
```

# Link to overall scores

```{r AthleteScorePCA, echo=FALSE}
pca.ath <- dudi.pca(athletes, scan=FALSE, nf=2)
olympdf <- data.frame(pca1=pca.ath$li[,1], score=olympic$score, id=rownames(athletes))
p <- ggplot(olympdf,aes(x=score, y=pca1, label=id)) + geom_text()
p + stat_smooth(method="lm", se=FALSE)
```

<!-- (/Users/susan/gitbiobook/BioBook/Chap8-IntroMultivariate/figure/chap8-AthleteScorePCA.png) -->

*Scatterplot of the scores given as a supplementary variable and the first principal component;  
the points are labeled by their order in the data set.*

# PCA as an exploratory tool: <br> using meta-information

```{r tcellexpr}
## center and scale the data 
## (they have already had variance normalization applied to them)
res.Msig3 <- dudi.pca(Msig3transp, center=TRUE, scale=TRUE, scannf=FALSE, nf=4)
screeplot(res.Msig3, main="")
```

## Plot by cell types

```{r tcelltypes}
celltypes <- factor(substr(rownames(Msig3transp), 7, 9))
table(celltypes)
```

```{r tcelltypes-plot}
#| fig-asp: 0.4
#| layout-ncol: 2
#| output-location: column
require(ggplot2)
gg <- cbind(res.Msig3$li, Cluster=celltypes)
gg <- cbind(sample=rownames(gg), gg)
ggplot(gg, aes(x=Axis1, y=Axis2)) + 
  geom_point(aes(colour=factor(Cluster)), size=2) + 
  geom_hline(yintercept=0, linetype=2) + 
  geom_vline(xintercept=0, linetype=2) +
  scale_color_discrete(name="Cluster") +
  coord_fixed() + xlim(-14,18) + ylim(-8,8)
```

*PCA of gene expression for a subset of 156 genes involved in specificities  
of each of the three separate T cell types: effector, naive and memory*

# Mass Spectroscopy Data Analysis

Example from  paper: [Kashnap et al, PNAS, 2013](http://www.pnas.org/content/110/42/17059.full)

```{r, echo=FALSE, eval=FALSE}
###Just for record, this is how the matrix was made
require(xcms)
load("data/xset3.RData")
mat1 <- groupval(xset3, value="into")
##
head(mat1)
dim(mat1)
## Matrix with with samples in rows and variables as columns
tmat <- t(mat1)
head(tmat[, 1:10])
logtmat <- log(tmat+1)
```

# Sample situations in PC map

```{r VanillaGGplot,out.width="800px"}
#| fig-asp: 0.7
#| layout-ncol: 2
#| output-location: column
##  PCA Example
require(ade4)
require(ggplot2)
load("resources/logtmat.RData")
pca.result <- dudi.pca(logtmat, scannf=FALSE, nf=3)
labs <- rownames(pca.result$li)
nos <- substr(labs, 3, 4)
type <- as.factor(substr(labs, 1, 2))
kos <- which(type == "ko")
wts <- which(type == "wt")
pcs <- data.frame(
  Axis1=pca.result$li[, 1], 
  Axis2=pca.result$li[, 2], labs, type)

pcsplot <- ggplot(pcs, aes(x=Axis1, y=Axis2, 
  label=labs, group=nos, colour=type)) + 
  geom_text(size=4, vjust=-0.5) + geom_point() 
pcsplot + geom_hline(yintercept=0, linetype=2) + 
  geom_vline(xintercept=0, linetype=2) +
  coord_fixed() + ylim(-12,18) 
```

# Extra Connections

```{r RedConnects,out.width="800px"}
pcsplot + geom_line(colour="red") + coord_fixed() + ylim(-12,18)
```

## Checking data by frequent multivariate projections

Phylochip data allowed us to discover a batch effect (phylochip).

![Phylochip data for three different batches and two different arrays, first principal plane explains 66\% of the total variation.](resources/ThreeSets28s.png)

## Weighted PCA

Sometimes we want to see variability between different groups or observations but need to weight them.    
This can happen when wanting to summarize data for heterogeneous groups with unequal sizes. 

Let's do this for the specific example of the Hiiragi (Ohnishi2014) data  
we saw in Lectures 3 and 5 and show how reweighting is relevant here.

```{r prepareData, message=FALSE}
library("Hiiragi2013")
set.seed(2013)
data("x")
FGF4probes <- (fData(x)$symbol == "Fgf4")
groups <- split(seq_len(ncol(x)), pData(x)$sampleGroup)
safeSelect <- function(grpnames) {
  stopifnot(all(grpnames %in% names(groups)))
  unlist(groups[grpnames])
}
g <- safeSelect(c("E3.25",
                  "E3.5 (EPI)", "E3.5 (PE)",
                  "E4.5 (EPI)", "E4.5 (PE)"))
nfeatures <- 100
varianceOrder <- order(rowVars(exprs(x[, g])), decreasing=TRUE)
varianceOrder <- setdiff(varianceOrder, which(FGF4probes))
selectedFeatures <- varianceOrder[seq_len(nfeatures)]
sampleColourMap <- setNames(
  unique(pData(x)$sampleColour), 
  unique(pData(x)$sampleGroup))
xwt <- x[selectedFeatures, g]
tab <- table(xwt$sampleGroup)
tab
```

--- 

We want to do a PCA on 66 points from the wild type genotype data, but the groups     
are not equally represented, so we will reweight them to even out the representations.

```{r}
selectedSamples <- with(pData(x), genotype == "WT")
xe <- x[, selectedSamples]
## To account for the different numbers in the groups, we reweight the samples
wt <- c(rep(1,36), rep(36/11,11), rep(36/11,11), rep(36/4,4), rep(36/4,4))
length(wt)
```

```{r}
## reweighted of groups using 'dudi.pca'
library("factoextra")
dfx <- data.frame(t(exprs(xwt)))
resPCAD <- dudi.pca(dfx, row.w=wt, center=TRUE, scale=TRUE, nf=2, scannf=FALSE)
```

<br> 

:::: {layout-ncol=2}

::: {.column}

```{r resPCADscree,fig.width=3.5,fig.height=3.5}
fviz_eig(resPCAD)
```

:::

::: {.column}

```{r resPCADplot,fig.width=7,fig.height=3.5,warning=FALSE}
fviz_pca_ind(resPCAD, 
  habillage=xwt$sampleGroup,
  col.ind=xwt$sampleColour,
  geom="point") 
```

:::

:::: 

# Summary of this Lecture

* Multivariate data require `conscious` preprocessing,  
  to make their variances comparable and their centers at the origin.
*  When data are matrices with many variables of numerical values,  
  we can still make useful graphical representations by making projections  
  on lower dimensions (planes and 3D are the most frequently used).
* PCA searches for new `more informative` variables  
  which are linear combinations of the old ones.
* PCA is based on finding decompositions of the matrix $X$  
  called SVD, this is equivalent to the eigenanalysis of $X'X$.  
  The squares of the singular values are the equal to the  
  eigenvalues and to the variances of the new variables. 
* Choosing k: You need to plot the variances/eigenvalues before you  
  decide how many axes are necessary to reproduce the signal in the data.
* Interpretation of PCA is facilitated by redundant  
  or contiguous meta-data about the observations.

# More examples of <br> supplementary variables

## One categorical variable: projcet the mean points

```{r WineBiplot}
require(ggbiplot)
load("data/wine.RData")
load("data/wineClass.RData")
wine[1:3, 1:7]
heatmap(1-cor(wine))
wine.pca <- prcomp(wine, scale.=TRUE)
table(wine.class)
fviz_pca_biplot(wine.pca, 
  habillage=wine.class, 
  addEllipses=TRUE, 
  circle=TRUE)
```

### Projecting Ellipses

We'll see later when we look at Microbiome data that sometimes, 
this projection can be problematic.

# Percentage of Inertia

```{r athletesbiplot}
require(ade4)
res.ath <- dudi.pca(athletes, nf=2, scannf=FALSE)
inertia.dudi(res.ath,col.inertia=TRUE)
```

Contributions are printed in 1/10000 and the sign is the sign of the coordinate.
