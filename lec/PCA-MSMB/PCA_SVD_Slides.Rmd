---
title: "Multivariate Analysis"
author: "Susan Holmes and Wolfgang Huber"
date: "July, 6 2017"
output: 
  slidy_presentation:
    highlight: pygments
---





Multivariate Analysis 
=======================
Susan Holmes and Wolfgang Huber (c)
July, 8 2017

<img src="http://web.stanford.edu/class/bios221/images/TheMatrix.jpg" alt="Decompose1" align="middle" style="width: 800px;"/>

```{r initial,message=FALSE,echo=FALSE, include=FALSE}
require(knitr)
require(ggplot2)
library(grid)
options(digits=3, width=80, prompt=" ", continue=" ")
opts_chunk$set(tidy = FALSE, dev = "png", dpi=160,
               dev.args = list(pointsize=18),
               fig.width = 5, fig.height = 5, fig.align = "center",
               cache=FALSE, out.width='550px',
#               out.width = '0.7\\textwidth',
 message=TRUE, error=TRUE)
####YOU NEED TO CHANGE THIS TO YOURS##############
#setwd("/Users/susan/Dropbox/summer14/STAMPS/Lect_Multi/")
```

Matrices and their Motivation
=============================

It is current practice to measure many variables on the same patients,
we may have all the biometrical characteristics, height, weight, BMI,
age as well as clinical variables such as blood pressure, blood sugar,
heart rate for 100 patients, these variables will not be independent.


What are the data?
======================

-   To start off, a useful toy example we’ll use is from the sports
    world; performances of decathlon athletes.

    These are measurements of athletes’ performances in the decathlon:
    the variables m100, m400, m1500 are performance times in seconds for
    the 100 metres, 400 metres and 1500 meters respectively, ‘m110‘ is
    the time taken to finish the 110 meters hurdles whereas pole is the
    pole-jump height, weight is the length in metres the athletes threw
    the weight.

             m100   long   weight   highj    m400    m110    disc   pole    jave    m1500
      --- ------- ------ -------- ------- ------- ------- ------- ------ ------- --------
        1   11.25   7.43    15.48    2.27   48.90   15.13   49.28   4.70   61.32   268.95
        2   10.87   7.45    14.97    1.97   47.71   14.46   44.36   5.10   61.76   273.02
        3   11.18   7.44    14.20    1.97   48.29   14.81   43.66   5.20   64.16   263.20
        4   10.62   7.38    15.02    2.03   49.06   14.72   44.80   4.90   64.04   285.11


```{r loadath,echo=FALSE}
library(ade4)
data(olympic)
athletes=olympic$tab
#athletes=scale(athletes)
#athletes=data.frame(athletes)
names(athletes)=c("m100","long","weight","highj","m400","m110","disc", "pole","javel","m1500")
```


Diabetes
===========
-   Clinical measurements (‘diabetes‘ data). This data measures glucose
    levels in the blood after fasting (glufast), after a test condition
    (glutest) as well as steady state plasma glucose (steady) and steady
    state (insulin) for diabetes, the sixth variable is not
    continuous and is considered a *supplementary* variable as we will
    see.

```{r}
diabetes=read.table(url("http://bios221.stanford.edu/data/diabetes.txt"),header=TRUE,row.names=1)
diabetes[1:4,]
```

Microbial Ecology
======================
-   Operational Taxon Unit read counts in a microbial ecology study; the
    columns represent different ‘species’ of bacteria, the rows are
    labeled for the samples.

                     469478 208196 378462 265971 570812
        EKCM1.489478      0      0      2      0      0
        EKCM7.489464      0      0      2      0      2
        EKBM2.489466      0      0     12      0      0
        PTCM3.489508      0      0     14      0      0
        EKCF2.489571      0      0      4      0      0

RNA-Seq
=============
-   Here are some RNA-seq transcriptomic data showing numbers of mRNA
    reads present for different patient samples, the rows are patients
    and the columns are the genes.

                   FBgn0000017 FBgn0000018 FBgn0000022 FBgn0000024 FBgn0000028 FBgn0000032
        untreated1        4664         583           0          10           0        1446
        untreated2        8714         761           1          11           1        1713
        untreated4        3150         310           0           3           0         672
        treated1          6205         722           0          10           0        1698
        treated3          3334         308           0           5           1         757

Mass Spectroscopy
====================
-   Mass spectroscopy data where we have samples containing informative
    labels (knockout versus wildtype mice) and protein $\times$ features
    designated by their m/z number.

        mz       129.9816   72.08144  151.6255  142.0349  169.0413    186.0355
        KOGCHUM1  60515      181495          0    196526    25500    51504.40
        WTGCHUM1 252579       54697        412    487800    48775    130491.15
        WTGCHUM2 187859       56318      46425   454226    45626    100845.01

Expression Data (microarray)
=============================

-   Here the rows are samples from different subjects and different T
    cell types and the columns are a subset of gene expression
    measurements on the 156 most differentially expressed genes
    (Holmes2005memory).
```{r}
#######Melanoma/Tcell Data: Peter Lee, Susan Holmes, PNAS.
load(url("http://bios221.stanford.edu/data/Msig3transp.RData"))
round(Msig3transp,2)[1:5,1:6]
celltypes=factor(substr(rownames(Msig3transp),7,9))
status=factor(substr(rownames(Msig3transp),1,3))
```

The voting data
======================
```{r votingdata}
#house=read.table("/Users/susan/Dropbox/CaseStudies/votes.txt")
#head(house[,1:5])
#party=scan("/Users/susan/Dropbox/CaseStudies/party.txt")
#table(party)
```

Biometrical Measurements
===========================
-   Measurements: turtles

```{r}
#require(MSBdata)
turtles=read.table(url("http://bios221.stanford.edu/data/PaintedTurtles.txt"),header=TRUE)
turtles[1:4,]
```

-   Some biological traits of lizards are available in the ‘ade4‘
    package

```{r}
require(ade4)
data(lizards)
lizards$traits[1:4,c(1,5,6,7,8)]
```


Data visualization and preparation
=======================================

It is always beneficial to start a multidimensional analysis by checking
the simple one dimensional and two dimensional summary statistics, we
can visualize these using a graphics package that builds on ‘ggplot2‘
called ‘GGally‘.


# Low dimensional data summaries and preparation

What do we mean by low dimensional ?

![flatland](https://imgs.xkcd.com/comics/flatland.png){width="80.00000%"}

If we are studying only one variable, just one column of our matrix, we
might call it ${\mathbf x}$ or ${\mathbf x}_{\bullet j}$; we call it one
dimensional.

A *one dimensional summary* a histogram that shows that
variable’s distribution, or we could compute its mean $\bar{x}$ or
median, these are zero-th dimensional summaries of one dimension data.

In lecture  3  we studied two dimensional scatterplots. 

When considering two variables ($x$ and $y$) measured
together on a set of observations, the  **correlation
coefficient**  measures how the variables co-vary.

This is a
single number summarizes two dimensional data, its formula involves
$\bar{x}$ and $\bar{y}$:
$$\hat{\rho}=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}
\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}
\label{eq:corrcoeff}$$ 

```{r corrturt}
cor(turtles[,-1])
```

```{r turtlespairs,fig.height=3,fig.width=3,fig.cap="Pairs plot for turtles data"}
library("GGally")
ggpairs(turtles[,-1],axisLabels = "none")
```



```{r PairsAthletes,fig.width=4.5,fig.height=4.5,fig.cap="Pairs athletes"}
ggpairs(athletes)
```

```{r heatmapathletes, fig.width=4.2,fig.height=3.5,fig.cap="Heatmap athletes"}
library("pheatmap")
pheatmap(cor(athletes),cell.width=10,cell.height=10)
```


## Preprocessing the data

We usually  **center**  the cloud of points around the
origin; the most common way of doing this is to make new variables whose
means are all zero. 

More robust scaling can be done also (median).


Different variables are measured in
different units, and at different scales, and so would be hard to
compare in their original form.

```{r turtlesmean}
library("ggplot2")
library("factoextra")
apply(turtles[,-1],2,sd)
apply(turtles[,-1],2,mean)
```

Transform the data: standardizing
the data to a common standard deviation is the usual transformation.
As in the correlation coefficient.


This rescaling is done using the  scale  function which makes
every column have a variance of 1.

 
```{r turtlesDim}
turtleMatScale=scale(turtles[,-1])
scaledturtles=data.frame(turtleMatScale,sex=turtles[,1])
apply(scaledturtles[,-4],2,mean)
apply(scaledturtles[,-4],2,sd)
```

```{r turtlesDim12}
ggplot(scaledturtles,aes(x=width,y=height, group =sex)) +
  geom_point(aes(color=sex))
```



A Little History
====================
Invented in 1901 by Karl Pearson as a way to reduce a two variable scatterplot to
a single coordinate.

Used by statisticians in the 1930s to
summarize a battery of psychological tests run on the same subjects
Hotelling:1933, extracting overall scores that could summarize many
variables at once.

It is called Principal Component Analysis
(abbreviated PCA). 

**Not principled**

Dimension reduction
===================

PCA is an ‘unsupervised learning technique’ because it treats all
variables as having the same status.

PCA is
visualization technique which produces maps of both variables and observations.



We are going to give you a flavor of what is called multivariate
analyses. As a useful first approximation we formulate many of the
methods through manipulations called linear algebra.


The *raison d’être* for multivariate analyses is connections or
associations between the different variables. 

If the columns of the
matrix are unrelated, we should just study each column separately and do
standard univariate statistics on them one by one.

Use geometry:

![projectionvector](http://bios221.stanford.edu/images/projectionvector.jpg)

## Low Dimensional Projections

Here we show one way of projecting two dimensional data onto a line. 

The
olympic data come from the ade4 package, they are the performances of
decathlon athletes in an olympic competition.


```{r SS,echo=FALSE}
require(ade4)
require(ggplot2)
require(grid)
data(olympic)
athletes=olympic$tab
athletes=scale(athletes)
athletes=data.frame(athletes)
names(athletes)=c("m100","long","weight","highj","m400","m110","disc", "pole","javel","m1500")
p <- ggplot(athletes, aes(x=weight, y=disc))
p=p + geom_point(size=3,shape=21)
pr=p + geom_rug(sides="b",colour="red")
print(pr)
```

Scatterplot of two variables showing projection on the x
coordinate in red.

How do we summarize two dimensional data by a line?
======================================================
In general, we lose information about the points when we project down
from two dimensions (a plane) to one (a line). 

If we do it just by using
the original coordinates, for instance the x coordinate as we did above,
we lose all the information about the second one.

There are actually
many ways of projecting the point cloud onto a line. One is to use what
are known as regression lines. Let’s look at these lines and how there
are constructed in R:

Regressing one variable on the other
======================================================

## The disc variable on the weight

```{r Reg1}
attach(athletes)
require(ggplot2)
reg1 <- lm(disc~weight,data=athletes)
#abline(reg1, col='red')
a <- reg1$coefficients[1] # Intercept
b <- reg1$coefficients[2] # slope
pline=p+geom_abline(intercept=a,slope=b, col="blue")
proj=pline+geom_segment(aes(x=weight, xend=weight, y=disc, 
yend=reg1$fitted),linetype=1,colour="red",
arrow = arrow(length = unit(0.15,"cm")))
print(proj)
```
 
The blue line minimizes the
sum of squares of the vertical residuals (in red), 
 
What is the variance of the points along the blue line?

```{r}
matproj=cbind(weight,reg1$fitted)
sum(apply(matproj,2,var))
```

## Regression of weight on discus

```{r Reg2,echo=FALSE}
require(ggplot2)
reg2 <- lm(weight~disc,data=athletes)
#abline(reg1, col='red')
a2 <- reg2$coefficients[1] # Intercept
b2 <- reg2$coefficients[2] # slope
#abline(-a/b , 1/b, col="blue")
pline2=p+geom_abline(intercept=-a2/b2,slope=1/b2, col="green",linetype=1,lwd=2)
proj2=pline2+geom_segment(aes(x=weight, xend=reg2$fitted, y=disc, yend=disc),
         linetype=1,colour="orange", arrow = arrow(length = unit(0.15,"cm")))
print(proj2)
```

Variance of points
```{r}
matproj2=cbind(weight,reg2$fitted)
sum(apply(matproj,2,var))
```


The
orange line minimizes the horizontal residuals for the weight variable
in orange.


# The PCA line: it minimizes in both directions


```{r PCAmin, fig.width=4, fig.height=4}
xy=cbind(athletes$disc,athletes$weight)
svda=svd(xy)
pc = xy %*% svda$v[,1] %*% t(svda$v[,1])
bp = svda$v[2,1] /svda$v[1,1]
ap = mean(pc[,2])-bp*mean(pc[,1])
p+geom_segment(xend=pc[,1],yend=pc[,2])+
geom_abline(intercept=ap,slope=bp, col="purple",lwd=1.5)
```

```{r PCAR1R2,fig.width=4,fig.height=4,echo=FALSE}
pline+geom_segment(aes(xend=weight, yend=reg1$fitted),colour="blue",alpha=0.35)+
      geom_abline(intercept=-a2/b2,slope=1/b2, col="green",lwd=1.5,alpha=0.8) +
      geom_segment(aes(xend=reg2$fitted, yend=disc), colour="orange",alpha=0.35) +
      geom_abline(intercept=ap,slope=bp, col="purple",lwd=1.5,alpha=0.8)+
      geom_segment(xend=pc[,1],yend=pc[,2],colour="purple",alpha=0.35)+coord_fixed()
```

The purple line minimizes both residuals and thus (through Pythagoras)
it minimizes the sum of squared distances from the points to the line.



*Minimizing the distance to the line in both directions, the
purple line is the principal component line, the green and blue line are
the regression lines.*



Variance along the line
========================





The lines created here are sensitive to the choice of units;
because we have made the standard deviations equal to one
for both variables, the PCA line is the diagonal
that cuts exactly in the middle of both
regression lines. 

The data were centered by
subtracting their means thus ensuring that the
line passes through the origin $(0,0)$.


Compute the variance of the points on the purple line.

The coordinates of the points
when we made the plot, these are in the `pc`
vector:
```{r PCAvari}
apply(pc,2,var)
sum(apply(pc,2,var))
```



PCA for 2 dimensional data
===========================

```{r CompareSDs}
ppdf = data.frame(PC1n=-svda$u[,1]*svda$d[1],PC2n=svda$u[,2]*svda$d[2])
ggplot(ppdf,aes(x=PC1n,y=PC2n))+geom_point()+ ylab("PC2 ")+
     geom_hline(yintercept=0,color="purple",lwd=1.5,alpha=0.5) +
      geom_point(aes(x=PC1n,y=0),color="red")+ xlab("PC1 ")+
     xlim(-3.5, 2.7)+ylim(-2,2)+coord_fixed() +
     geom_segment(aes(xend=PC1n,yend=0), color="red")
```





```{r CompareSDblue,echo=FALSE}
segs1=apply(cbind(rep(0,33),ppdf[,1]),1,min)
segs2=apply(cbind(rep(0,33),ppdf[,1]),1,max)
segm=data.frame(xmin=segs1,xmax=segs2, yp=seq(-1,-2,length=33),yo=ppdf$PC2n)
ggplot(ppdf,aes(x=PC1n,y=PC2n))+geom_point()+ ylab("PC2 ")+ xlab("PC1 ")+
     geom_hline(yintercept=0,color="purple",lwd=1.5,alpha=0.5) +
      geom_point(aes(x=PC1n,y=0),color="red")+
     xlim(-3.5, 2.7)+ylim(-2,2)+coord_fixed() +
     geom_segment(aes(xend=PC1n,yend=0), color="red")+
  geom_segment(data=segm,aes(x=xmin,xend=xmax,y=yo,yend=yo), color="blue",alpha=0.5)
```



Notes about Lines
===============================
The line created here is sensitive to the choice of units,
and to the center of the cloud.

Note that Pythagoras’ theorem tells us two interesting things here, if
we are minimizing in both horizontal and vertical directions we are in
fact minimizing the diagonal projections onto the line from each point.


Principal Components are Linear Combinations of the ‘old’ variables
===================================================================

To understand what that a linear combination really is, we can take an
analogy, when making a healthy juice mix, you can follow a recipe.

![image](http://web.stanford.edu/class/bios221/images/Vegetable-Juice.jpg)


![image](http://web.stanford.edu/class/bios221/images/RecipeVeggie.jpg)

$$V=2\times \mbox{ Beets }+ 1\times \mbox{Carrots } +\frac{1}{2}
\mbox{ Gala}+ \frac{1}{2} 
\mbox{ GrannySmith}
+0.02\times \mbox{ Ginger} +0.25 \mbox{ Lemon }$$ This recipe is a
linear combination of individual juice types, in our analogy these are
replaced by the original variables. The result is a new variable, the
coefficients $(2,1,\frac{1}{2},\frac{1}{2},0.02,0.25)$ are called the
loadings.

Optimal lines
-------------

A linear combination of variables defines a line in our space in the
same way we say lines in the scatterplot plane for two dimensions. As we
saw in that case, there are many ways to choose lines onto which we
project the data, there is however a ‘best’ line for our purposes.

**Total variance can de decomposed** The total sums of squares of the
distances between the points and any line can be decomposed into the
distance to the line and the variance along the line.

We saw that the principal component minimizes the distance to the line,
and it also maximizes the variance of the projections along the line.

Good Projections
======================
![image](http://web.stanford.edu/class/bios221/images/CAM3.png)
What is this?

Good Projections
======================
![MysteryImage](http://web.stanford.edu/class/bios221/images/CAM4.png)




Which projection do you think is better?


It’s the projection that maximizes the area of the shadow and an
equivalent measurement is the sums of squares of the distances between
points in the projection, we want to see as much of the variation as
possible, that’s what PCA does.

# The PCA workflow

![Many Choices have to made during PCA processing.](http://web.stanford.edu/class/bios221/images/smallorgacp1.png)

PCA is based on the principle of finding the largest axis of
inertia/variability and then iterating to find the next best axis which
is orthogonal to the previous one and so on. 



The Inner Workings of PCA: the Singular Value Decomposition
=================================================================
Eigenvalues of X'X or Singular values of X tell  us the rank.

What does rank mean?

```
   X |  2  4  8  
  ---| --------
   1 | 
   2 |
   3 |
   4 |
```
```
  X  |  2  4  8  
  -- | ---------
  1  |  2
  2  |  4
  3  |  6
  4  |  8
```
```
  X  |  2  4  8  
  ---| --------
   1 |  2  4  8
   2 |  4  8 16
   3 |  6 12 24
   4 |  8 16 32 

```
We say that the matrix 
$$\begin{pmatrix}
2&4&8\\ 4&8&16\\
6 &12&24\\
8&16&32\\
\end{pmatrix}
$$
is of rank one.

$$\begin{pmatrix}
2&4&8\\ 4&8&16\\
6 &12&24\\
8&16&32\\
\end{pmatrix}=
=u * t(v)= u * v', \qquad
u =\left(\begin{smallmatrix}
1 \\2\\ 3\\ 4
\end{smallmatrix}\right) \mbox{ and }
v'=t(v)=(2\; 4\; 8) .
$$

Backwards from the matrix to decomposition
===========================================
```{r}
X=matrix(c(780, 75, 540, 936, 90, 648, 1300, 125, 900,
          728, 70, 504),nrow=3)
X
u1=c(0.8,0.1,0.6)
v1=c(0.4,0.5,0.7,0.4)
sum(u1^2)
sum(v1^2)
s1=2348.2
s1*u1 %*%t(v1)
X-s1*u1 %*%t(v1)
```

Graphical Decompositions
============================


```{r}
# <img src="/Users/susan/Dropbox/images/testsmallmosaic.jpg" alt="Decompose1" style="width: 400px;"/>
# 
# Matrix $X$ we would like to decompose.
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot1.png" alt="Decompose1" style="width: 400px;"/>
# 
# Areas are proportional to the  entries
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot2.png" alt="Decompose2" style="width: 400px;"/>
# 
# Looking at different possible margins
# 
# <img src="/Users/susan/Books/CUBook/images/SVD-mosaicXplot3.png" alt="Decompose3" style="width: 400px;"/>
```


Forcing the margins to have norm $1$


Check with R
==================

```{r}
## ----checkX--------------------------------------------------------------
u1=c(0.8196, 0.0788, 0.5674)
v1=c(0.4053, 0.4863, 0.6754, 0.3782)
s1=2348.2
s1*u1 %*%t(v1)
```

```{r}
Xsub=matrix(c(12.5 , 35.0 , 25.0 , 25,9,14,26,18,16,21,49,
           32,18,28,52,36,18,10.5,64.5,36),ncol=4,byrow=T)
Xsub
USV=svd(Xsub)
USV
```

```{r}
## ----CheckUSV------------------------------------------------------------
Xsub-(135*USV$u[,1]%*%t(USV$v[,1]))
Xsub-(135*USV$u[,1]%*%t(USV$v[,1]))-(28.1*USV$u[,2]%*%t(USV$v[,2]))
Xsub- USV$d[1]*USV$u[,1]%*%t(USV$v[,1])-USV$d[2]*USV$u[,2]%*%t(USV$v[,2])
```


Another Example
====================

```{r}
Xsub=matrix(c(12.5 , 35.0 , 25.0 , 25,9,14,26,18,16,21,49,32,18,28,52,36,18,10.5,64.5,36),ncol=4,byrow=T)
Xsub
svd(Xsub)
```

```{r}
USV=svd(Xsub)
XS1=Xsub-USV$d[1]*(USV$u[,1]%*% t(USV$v[,1]))
XS1
XS2=XS1-USV$d[2]*(USV$u[,2]%*% t(USV$v[,2]))
XS2
```

Special Example of Rank one matrix: independence
==================================================
```{r}
require(ade4)
HairColor=HairEyeColor[,,2]
HairColor
chisq.test(HairColor)
prows=sweep(HairColor,1,apply(HairColor,1,sum),"/")
pcols=sweep(HairColor,2,apply(HairColor,2,sum),"/")
Indep=313*as.matrix(prows)%*%t(as.matrix(pcols))
round(Indep)
sum((Indep-HairColor)^2/Indep)
```

SVD for real data
=====================
```{r}
## ------------------------------------------------------------------------
diabetes.svd=svd(scale(diabetes[,-5]))
names(diabetes.svd)
diabetes.svd$d
```

```{r}
turtles.svd=svd(scale(turtles[,-1]))
turtles.svd$d
```

SVD
====

```{r fig.width=10, fig.height=6,echo=FALSE}
library(png)
library(grid)
#img <- readPNG("/Users/susan/Books/CUBook/images/SumRankOneD.png")
 #grid.raster(img)
```

$${\Large X\qquad = u_{\bullet 1} * s_1 * v_{\bullet 1}\qquad + \qquad u_{\bullet 2} * s_2 * v_{\bullet 2}\qquad + \qquad u_{\bullet 3} * s_3 * v_{\bullet 3}+}$$

We write our horizontal/vertical decompostion of the matrix
$X$ in short hand as:
$${\Large X = USV', V'V=I, U'U=I, S} \mbox{ diagonal matrix of singular values, given by the {\tt d} component in the R function}$$
The crossproduct of X with itself verifies
$${\Large X'X=VSU'USV'=VS^2V'=V\Lambda V'}$$
where $V$ is called the eigenvector matrix of the symmetric
matrix $X'X$ and $\Lambda$ is the diagonal matrix of
eigenvalues of $X'X$.

Why Eigenvectors are useful?
=============================================
![Why would eigenvectors come into use in Cinderella?](http://web.stanford.edu/class/bios221/images/xkcdEigenVectors.png)

[Khan's Academy](https://www.khanacademy.org/math/linear-algebra/alternate_bases/eigen_everything/v/linear-algebra--introduction-to-eigenvalues-and-eigenvectors)



Principal Components
============================

The singular vectors from the singular value decomposition,
`svd` function above
tell us the coefficients to put in front of the old variables to
make our new ones with better properties.
We write this as :
$$PC_1=c_1 X_{\bullet 1} +c_2 X_{\bullet 2}+ c_3 X_{\bullet 3}+\cdots c_p X_{\bullet p}$$

Replace $X_{\bullet 1},X_{\bullet 2}, \ldots X_{\bullet p}$
 by 
$$PC_1, PC_2, \ldots PC_k$$

# What is the largest k can be ?

Suppose we have 5 samples with 23,000 genes measured on them, what is the dimensionality of these 
data?






The number of principal components is less than or equal to the number of original variables. 
$$K\leq min(n,p)$$

The geometr(ies) of data: good trick look at size of vectors:

```{r fig.width=9, fig.height=4,echo=FALSE}
library(png)
library(grid)
#img <- readPNG("/Users/susan/Books/CUBook/images/DataCloudGeometry.png")
# grid.raster(img)
```

```{r fig.width=9, fig.height=4,echo=FALSE}
# library(png)
# library(grid)
# img <- readPNG("/Users/susan/Books/CUBook/images/DataCloudGeometry4.png")
#  grid.raster(img)
```




The Principal Component transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each successive component in turn has the highest variance possible under the constraint that it be orthogonal to the preceding components. 
$$\max_{aX} \mbox{var}(Proj_{aX} (X))$$

Suppose the matrix of data $X$ has been made to have column means 0
and standard deviations 1.

Matrix Decomposition
=========================

We call the principal components the 
columns of the matrix, $C=US$.

The columns of U (the matrix given as
USV\$u in the output from the svd function above) 
are rescaled to have norm $s^2$, 
the variance they are responsable for.


If the matrix $X$ comes from the study of $n$ different samples or specimens, then
the principal components provides new coordinates for these $n$ points
these are sometimes also called the scores in some of the (many)
PCA functions available in R
(`princomp`,`prcomp`,`dudi.pca` in `ade4`).



Transition Formulae
======================
If we only want the first one then it is just $c_1=s_1 u_1$.

Variance explained by first principal component: $s_1^2$:

Notice that $||c_1||^2=s_1'u_1 u_1' s_1= s_1^2 u_1'u_1=s_1^2=\lambda_1$

$$X'C=VSU'US=VS^2$$


Remarks:
------------------
- 1. Each principal component is chosen to maximize the variance it explains, this variance is measured
by the corresponding eigenvalue.      
- 2. The new variables are made to be orthogonal, if the data
are multivariate normal the new variables will be independent.      
- 3. When the variables are rescaled or we choose the correlation
matrix as the one we want to study instead of the covariance matrix
then the sum of the variances of all the variables is the number
of variables (=p), this is sometimes called the trace.       
- 4. The principal components are always ordered by
``importance'', always look at what proportion of the
variability you are interpreting (and check the screeplot before deciding how many components).




# A few examples of using PCA
We start with the turtles data that has 3 continuous variables and a gender variable that we leave out for the 
original PCA analysis.

## turtles Data

When computing the variance covariance matrix, many programs use 1/(n-1) as the denominator, here
n=48 so the sum of the variances are off by a small fudge factor of 48/47.
```{r PCAturtlesunscaled}
turtles3var=turtles[,-1]
apply(turtles3var,2,mean)
turtles.pca=princomp(turtles3var)
print(turtles.pca)
(25.06^2+2.26^2+1.94^2)*(48/47)
apply(turtles3var,2,var)
```
```{r PCAturtles}
apply(turtles[,-1],2,sd)
turtlesc=scale(turtles[,-1])
cor(turtlesc)
pca1=princomp(turtlesc)
pca1
```

Step one: always the screeplot
=================================
The screeplot showing the eigenvalues for the standardized data: one very large component in this case and two very small ones, the data are (almost) one dimensional.

```{r turtlesbiplot}
pca.turtles=dudi.pca(turtles[,-1],scannf=F,nf=2)
scatter(pca.turtles)
```

Why ?
-------

Choose k carefully:

```{r screeploteq, echo=FALSE, results=FALSE}
load("/Users/susan/Books/CUBook/data/screep7.RData")
pcaS7 = dudi.pca(screep7, scannf = F)
fviz_eig(pcaS7,geom="bar",width=0.3)
#problem with dudi and prcomp eigenvalues
#prcomp does not scale by default, dudi.pca does
#fviz_eig(pcaS7,geom="bar",width=0.3)
#p7=prcomp(screep7,scale= TRUE)
#p7$sdev^2
#plot(p7)
```




Step Two: Variables
========================
```{r turtlesCircle,echo=FALSE}
require(ggplot2)
circle <- function(center = c(0, 0), npoints = 100) {
    r = 1
    tt = seq(0, 2 * pi, length = npoints)
    xx = center[1] + r * cos(tt)
    yy = center[1] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}
corcir = circle(c(0, 0), npoints = 100)
pca1 = prcomp(turtlesc, scale. = TRUE)
# create data frame with correlations between variables and PCs
correlations = as.data.frame(cor(turtlesc, pca.turtles$li))

# data frame with arrows coordinates
arrows = data.frame(x1 = c(0, 0, 0), y1 = c(0, 0, 0), x2 = correlations$Axis1, 
    y2 = correlations$Axis2)

# geom_path will do open circles
ggplot() + geom_path(data = corcir, aes(x = x, y = y), colour = "gray65") + 
    geom_segment(data = arrows, aes(x = x1, y = y1, xend = x2, yend = y2), colour = "gray65") + 
    geom_text(data = correlations, aes(x = Axis1, y = Axis2, label = rownames(correlations))) + 
    geom_hline(yintercept = 0, colour = "gray65") + geom_vline(xintercept = 0, 
    colour = "gray65") + xlim(-1.1, 1.1) + ylim(-1.1, 1.1) + labs(x = "PC1 Axis", 
    y = "PC2 axis") + ggtitle("Circle of correlations")
```

Biplot
============
```{r turtlesbiplotfunction,echo=FALSE,out.width="810px",out.height="600px"}
PCbiplot <- function(outputfromdudi, x="Axis1", y="Axis2") {
    # outputfromdudi being a dudi object
    PC=outputfromdudi
    
    data <- data.frame(obsnames=row.names(PC$li), PC$li)
    plot <- ggplot(data, aes_string(x=x, y=y)) + geom_text(alpha=.4, size=3, aes(label=obsnames))
    plot <- plot + geom_hline(aes(0), size=.2) + geom_vline(aes(0), size=.2)
dimnames(PC$co)[[2]]=c(x,y)    
datapc <- data.frame(varnames=rownames(PC$co), PC$co)
    mult <- min(
        (max(data[,y]) - min(data[,y])/(max(datapc[,y])-min(datapc[,y]))),
        (max(data[,x]) - min(data[,x])/(max(datapc[,x])-min(datapc[,x])))
        )
    datapc <- transform(datapc,
            v1 = .8 * mult * (get(x)),
            v2 = .8 * mult * (get(y))
            )
    plot <- plot + coord_equal() + geom_text(data=datapc, aes(x=v1, y=v2, label=varnames), size = 5, vjust=1, color="red")
    plot <- plot + geom_segment(data=datapc, aes(x=0, y=0, xend=v1, yend=v2), arrow=arrow(length=unit(0.2,"cm")), alpha=0.75, color="red")
    plot
}
PCbiplot(pca.turtles)
```

All together
=====================
```{r scatterturtles}
scatter(pca.turtles)
```


Exercise: How are the following  numbers related?
```{r}
svd(turtlesc)$d/pca1$sdev
sqrt(47)
nrow(turtlesc)
```

# Lizards Data Analyses
This data  set describes  18 lizards as reported by Bauwens and D\'iaz-Uriarte (1997). It also gives life-history traits corresponding to these 18 species.

 - `mean.L` (mean length (mm)), `matur.L` (length at maturity (mm)),    
 - `max.L` (maximum length (mm)), `hatch.L` (hatchling length (mm)),     
 - `hatch.m` (hatchling mass (g)), `clutch.S` (Clutch size),      
 - `age.mat` (age at maturity (number of months of activity)),    
 - `clutch.F` (clutch frequency).

```{r, echo=TRUE}
library(ade4)
data(lizards)
names(lizards)
lizards$traits[1:4,]
```
It is always a good idea to check the variables one at a time and two at a time to see what the basic
statistics are for the data
```{r}
tabtraits=lizards$traits
options(digits=2)
colMeans(tabtraits)
cor(tabtraits)
```

Biplot
===========
```{r lizardbiplot}
require(ade4)
res=dudi.pca(tabtraits,scannf=F,nf=2)
barplot(res$eig)
res
biplot(res)
```

```{r}
res$eig/(sum(res$eig))
```

# The Decathlon Athletes

```{r ade4athletes}
round(cor(athletes),1)
pca.ath <- dudi.pca(athletes, scan = F)
pca.ath$eig
barplot(pca.ath$eig)
```

The screeplot is the first thing to look at, it tells us that it is satifactory to use
a two dimensional plot.

Correlation Circle
====================
```{r athletecorr}
s.corcircle(pca.ath$co,clab=1, grid=FALSE, fullcircle = TRUE,box=FALSE)
```


The correlation circle made by showing the projection of the old variables onto the two first new principal axes.


```{r atheleteneg}
athletes[,c(1,5,6,10)]=-athletes[,c(1,5,6,10)]
round(cor(athletes),1)
pcan.ath=dudi.pca(athletes,nf=2,scannf=F)
pcan.ath$eig
```


Now all the negative correlations are quite small ones.
Doing the screeplot over again will show no change in the eigenvalues, the only thing that changes is the sign of
loadings for the m variables.

New Data changing signs
==========================
```{r athletecorrn}
s.corcircle(pcan.ath$co,clab=1.2,box=FALSE)
```

*Correlation circle after changing the signs of the running variables.*


Observations
===============

```{r, athletepc,echo=FALSE,fig.width=4.2, fig.height =3.6}
ggplot(pcan.ath$l1,aes(x=RS1,y=RS2,label=rownames(pcan.ath$l1)))+geom_text() +
 geom_hline(yintercept=0,linetype=2) + xlim(-3,3) +ylim(-2.5,3) +
  geom_vline(xintercept=0,linetype=2) +coord_fixed()
```

```{r}
data(olympic)
olympic$score
```


Link to overall scores
=========================
```{r AthleteScorePCA,echo=FALSE}
pca.ath <- dudi.pca(athletes, scan = FALSE,nf=2)
olympdf=data.frame(pca1=pca.ath$li[,1],score=olympic$score,id=rownames(athletes))
p=ggplot(olympdf,aes(x=score, y=pca1,label=id))+geom_text()
p+stat_smooth(method="lm", se=FALSE)
```
(/Users/susan/gitbiobook/BioBook/Chap8-IntroMultivariate/figure/chap8-AthleteScorePCA.png)
*Scatterplot of the scores given as a supplementary variable and the first principal component, the points are labeled by their order in the data set.*

# PCA as an exploratory tool: using meta-information

```{r tcellexpr}
######center and scale the data 
###(they have already had variance normalization applied to them)
res.Msig3=dudi.pca(Msig3transp,center=TRUE,scale=TRUE,scannf=F,nf=4)
screeplot(res.Msig3,main="")
```

Plot by cell types
======================
```{r tcelltypes,out.width='500px'}
celltypes=factor(substr(rownames(Msig3transp),7,9))
table(celltypes)
status=factor(substr(rownames(Msig3transp),1,3))
require(ggplot2)
gg <- cbind(res.Msig3$li,Cluster=celltypes)
gg <- cbind(sample=rownames(gg),gg)
ggplot(gg, aes(x=Axis1, y=Axis2)) + 
  geom_point(aes(color=factor(Cluster)),size=5) + 
  geom_hline(yintercept=0,linetype=2) + 
  geom_vline(xintercept=0,linetype=2) +
  scale_color_discrete(name="Cluster") +
  coord_fixed()+ ylim(-8,+8)
  xlim(-14,18)
```

*PCA of gene expression for a subset of 156 genes involved in specificities
of each of the three separate T cell types: effector, naive and memory*

Mass Spectroscopy Data Analysis
========================================
Example from  paper:
[Kashnap et al, PNAS, 2013](http://www.pnas.org/content/110/42/17059.full)

```{r, echo=FALSE, eval=FALSE}
###Just for record, this is how the matrix was made
require(xcms)
load(url("http://bios221.stanford.edu/data/xset3.RData"))
mat1 =groupval(xset3, value="into")
##
head(mat1)
dim(mat1)
## Matrix with with samples in rows and variables as columns
tmat= t(mat1)
head(tmat[,1:10])
logtmat=log(tmat+1)
```


Sample situations in PC map
======================================
```{r VanillaGGplot,out.width="800px"}
##  PCA Example
require(ade4)
require(ggplot2)
load(url("http://bios221.stanford.edu/data/logtmat.RData"))
pca.result=dudi.pca(logtmat, scannf=F,nf=3)
labs=rownames(pca.result$li)
nos=substr(labs,3,4)
type=as.factor(substr(labs,1,2))
kos=which(type=="ko")
wts=which(type=="wt")
pcs=data.frame(Axis1=pca.result$li[,1],Axis2=pca.result$li[,2],labs,type)

pcsplot=ggplot(pcs,aes(x=Axis1,y=Axis2,label=labs,group=nos,colour=type)) + 
  geom_text(size=4,vjust=-0.5) + geom_point() 
 pcsplot +  geom_hline(yintercept=0,linetype=2)  +coord_fixed() + ylim(-12,18) +
  geom_vline(xintercept=0,linetype=2)
```

Extra Connections
=======================
```{r RedConnects,out.width="800px"}
pcsplot+geom_line(colour="red")  + coord_fixed()  + ylim(-12,18)
```


# Checking data by frequent multivariate projections

Phylochip data allowed us to discover a batch effect (phylochip).

![Phylochip data for three different batches and two different arrays, first principal plane explains 66\% of the total variation.](http://web.stanford.edu/class/bios221/images/ThreeSets28s.png)





# Weighted PCA

Sometimes we want to see variability between different
groups or observations but need to weight them. This can happen when
wanting to summarize data for heterogeneous groups for which do
not have equal sizes. Let's do this for the specific example


of the
Hiiragi (Ohnishi2014)
data  we saw in Lectures 3 and 5
and show how reweighting is relevant here.

```{r prepareData,message=FALSE}
library("Hiiragi2013")
set.seed(2013)
data("x")
FGF4probes = (fData(x)$symbol == "Fgf4")
groups = split(seq_len(ncol(x)), pData(x)$sampleGroup)
safeSelect = function(grpnames){
  stopifnot(all(grpnames %in% names(groups)))
  unlist(groups[grpnames])
}
g = safeSelect(c("E3.25",
                 "E3.5 (EPI)", "E3.5 (PE)",
                 "E4.5 (EPI)", "E4.5 (PE)"))
nfeatures = 100
varianceOrder = order(rowVars(exprs(x[, g])), decreasing = TRUE)
varianceOrder = setdiff(varianceOrder, which(FGF4probes))
selectedFeatures = varianceOrder[seq_len(nfeatures)]
sampleColourMap = setNames(unique(pData(x)$sampleColour), unique(pData(x)$sampleGroup))
xwt = x[selectedFeatures, g]
tab = table(xwt$sampleGroup)
tab
```

We want to do a PCA on 66 points from the wild type genotype
data,  but the groups are not equally represented,
so we will reweight them to even out the representations.

```{r}
selectedSamples = with(pData(x), genotype=="WT")
xe = x[, selectedSamples]
##To account for the different numbers in the groups, we reweight the samples
wt=c(rep(1,36), rep(36/11,11),rep(36/11,11),rep(36/4,4),rep(36/4,4))
length(wt)
dfx=data.frame(t(exprs(xwt)))
```


```{r resPCADscree,fig.width=3.5,fig.height=3.5}
library("factoextra")
## reweighted of groups using dudi.pca
resPCAD=dudi.pca(dfx,row.w=wt,center=TRUE,scale=TRUE,nf=2,scannf=F)
fviz_eig(resPCAD)
```
%#fviz_pca_ind(resPCAD,geom = c("point"),habillage=xwt$sampleGroup)
%##To use the special in house colours

```{r resPCADplot,fig.width=7,fig.height=3.5,warning=FALSE}
fviz_pca_ind(resPCAD,geom = c("point"), habillage = xwt$sampleGroup,
             col.ind=xwt$sampleColour)
```







# Summary of this Lecture

*  Multivariate data require `conscious` preprocessing,
 to make their variances comparable and their centers 
at the origin.
*  When data are matrices with many variables 
of numerical values, we can still make useful
graphical representations by making projections on
lower dimensions (planes and 3D are the most frequently used).
*  PCA searches for new `more informative` variables which are
linear combinations of the old ones.
*  PCA is based on finding decompositions of the matrix $X$ called SVD,
this is equivalent to the eigenanalysis of $X'X$.
The squares of the singular values are the 
equal to the eigenvalues and to the variances of the new variables.
* Choosing k: You need to plot the variances/eigenvalues before you decide how many axes are
necessary to reproduce the signal in the data.
*  Interpretation of PCA is facilitated by redundant or contiguous
meta-data about the observations.


# More examples of supplementary variables

## One categorical variable: projcet the mean points

```{r WineBiplot}
require(ggbiplot)
data(wine)
wine[1:3,1:7]
heatmap(1-cor(wine))
wine.pca <- prcomp(wine, scale. = TRUE)
table(wine.class)
fviz_pca_biplot(wine.pca, 
                habillage = wine.class, addEllipses = TRUE, circle = TRUE)
```

### Projecting Ellipses

We'll see later when we look at Microbiome data that sometimes,
this projection can be problematic.



Percentage of Inertia
========================


```{r athletesbiplot}
require(ade4)
res.ath=dudi.pca(athletes,nf=2,scannf=F)
inertia.dudi(res.ath,col.inertia=TRUE)
```

Contributions are printed in 1/10000 and the sign is the sign of the coordinate.


